{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leninvarma11/Homework-1-Lenin-Varma-Nallapu-700076011/blob/main/Homework-1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1 Question\n",
        " 1)Write a regex to find U.S. ZIP codes (disjunction + token boundaries) Match 12345 or 12345-6789 or 12345 6789 (hyphen or space allowed for the +4 part). Make sure you only match whole tokens (not inside longer strings).\n",
        "\n",
        "Ans:\\b\\d{5}(?:[ -]\\d{4})?\\b\n",
        "\n",
        "2)Negation in disjunction (word start rules) Find all words that do not start with a capital letter. Words may include internal apostrophes/hyphens like don’t, state-of-the-art.\n",
        "\n",
        "Ans: \\b(?![A-Z])[a-z]+(?:['’-][a-z]+)*\\b\n",
        "\n",
        "3)Convenient aliases (numbers, a bit richer) Extract all numbers that may have: optional sign (+/-), optional thousands separators (commas), optional decimal part, optional scientific notation (e.g., 1.23e-4).\n",
        "\n",
        "Ans: \\b[+-]?(?:(?:\\d{1,3}(?:,\\d{3})+|\\d+)(?:.\\d+)?|.\\d+)(?:[eE][+-]?\\d+)?\\b\n",
        "\n",
        "4)More disjunction (spelling variants) Match any spelling of “email”: email, e-mail, or e mail. Accept either a space or a hyphen (including en‐dash –) between e and mail, and be case-insensitive.\n",
        "\n",
        "Ans: (?i)\\bE(?:[- ]?)mail\\b With en- dash : (?i)\\bE(?:\\s|[-\\u2013])?mail\\b\n",
        "\n",
        "5 )Wildcards, optionality, repetition (with punctuation) Match the interjection go, goo, gooo, … (one or more o), as a word, and allow an optional trailing punctuation mark ! . , ? (e.g., gooo!).\n",
        "\n",
        "ANS: \\bgo+\\b[!.,?]?\n",
        "\n",
        "6)Anchors (line/sentence end with quotes)\n",
        "Match lines that end with a question mark possibly followed only by closing quotes/brackets like \")”’] and spaces.\n",
        "\n",
        "Ans :(?m)^[^\\n]?[)\"]\\u201D\\u2019\\s]$\n",
        "\n"
      ],
      "metadata": {
        "id": "XE7z86hUqGNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#2 Question\n",
        "\n",
        "import spacy\n",
        "\n",
        "# === 1. Paragraph ===\n",
        "paragraph = \"Yesterday, I went to New York City. It’s a busy place, full of energy! However, I didn’t stay long.\"\n",
        "\n",
        "# === 1a. Naïve space-based tokenization ===\n",
        "naive_tokens = paragraph.split()\n",
        "\n",
        "# === 1b. Manual correction (punctuation, clitics, MWEs) ===\n",
        "manual_tokens = [\n",
        "    'Yesterday', ',', 'I', 'went', 'to', 'New York City', '.',\n",
        "    'It', '’s', 'a', 'busy', 'place', ',', 'full', 'of', 'energy', '!',\n",
        "    'However', ',', 'I', 'did', 'n’t', 'stay', 'long', '.'\n",
        "]\n",
        "\n",
        "# === 2. Tool-based tokenization (spaCy) ===\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(paragraph)\n",
        "spacy_tokens = [t.text for t in doc]\n",
        "\n",
        "# === 3. MWEs ===\n",
        "mwes = [\n",
        "    \"New York City - place name, should be one token.\",\n",
        "    \"social media - fixed phrase with non-compositional meaning.\",\n",
        "    \"kick the bucket - idiom meaning 'to die'.\"\n",
        "]\n",
        "\n",
        "# === 4. Reflection text ===\n",
        "reflection = \"\"\"The hardest part of tokenization was handling contractions and MWEs.\n",
        "For example, deciding whether to split 'didn’t' depends on whether the task requires grammatical structure or surface forms.\n",
        "Compared with English, some languages are harder because of rich morphology and agglutination, where one word encodes multiple morphemes.\n",
        "Punctuation is relatively easy but still introduces edge cases, like apostrophes.\n",
        "Morphology and MWEs make tokenization more difficult because they require semantic knowledge beyond surface rules.\n",
        "Overall, tokenization is a balance between linguistic accuracy and tool consistency.\n",
        "\"\"\"\n",
        "\n",
        "# === Print Outputs ===\n",
        "print(\"=== 1a. Naïve space-based tokens ===\")\n",
        "print(naive_tokens, \"\\n\")\n",
        "\n",
        "print(\"=== 1b. Manual corrected tokens ===\")\n",
        "print(manual_tokens, \"\\n\")\n",
        "\n",
        "print(\"=== 2. spaCy tokens ===\")\n",
        "print(spacy_tokens, \"\\n\")\n",
        "\n",
        "print(\"=== Differences (Manual vs spaCy) ===\")\n",
        "print(\"In manual but not spaCy:\", set(manual_tokens) - set(spacy_tokens))\n",
        "print(\"In spaCy but not manual:\", set(spacy_tokens) - set(manual_tokens), \"\\n\")\n",
        "\n",
        "print(\"=== 3. Multiword Expressions (MWEs) ===\")\n",
        "for m in mwes:\n",
        "    print(\"-\", m)\n",
        "print()\n",
        "\n",
        "print(\"=== 4. Reflection ===\")\n",
        "print(reflection)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QK-8nicbj0ZB",
        "outputId": "037c9f8d-2cfe-4df9-d9c7-ff08ff3d2756"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1a. Naïve space-based tokens ===\n",
            "['Yesterday,', 'I', 'went', 'to', 'New', 'York', 'City.', 'It’s', 'a', 'busy', 'place,', 'full', 'of', 'energy!', 'However,', 'I', 'didn’t', 'stay', 'long.'] \n",
            "\n",
            "=== 1b. Manual corrected tokens ===\n",
            "['Yesterday', ',', 'I', 'went', 'to', 'New York City', '.', 'It', '’s', 'a', 'busy', 'place', ',', 'full', 'of', 'energy', '!', 'However', ',', 'I', 'did', 'n’t', 'stay', 'long', '.'] \n",
            "\n",
            "=== 2. spaCy tokens ===\n",
            "['Yesterday', ',', 'I', 'went', 'to', 'New', 'York', 'City', '.', 'It', '’s', 'a', 'busy', 'place', ',', 'full', 'of', 'energy', '!', 'However', ',', 'I', 'did', 'n’t', 'stay', 'long', '.'] \n",
            "\n",
            "=== Differences (Manual vs spaCy) ===\n",
            "In manual but not spaCy: {'New York City'}\n",
            "In spaCy but not manual: {'City', 'York', 'New'} \n",
            "\n",
            "=== 3. Multiword Expressions (MWEs) ===\n",
            "- New York City - place name, should be one token.\n",
            "- social media - fixed phrase with non-compositional meaning.\n",
            "- kick the bucket - idiom meaning 'to die'.\n",
            "\n",
            "=== 4. Reflection ===\n",
            "The hardest part of tokenization was handling contractions and MWEs. \n",
            "For example, deciding whether to split 'didn’t' depends on whether the task requires grammatical structure or surface forms. \n",
            "Compared with English, some languages are harder because of rich morphology and agglutination, where one word encodes multiple morphemes. \n",
            "Punctuation is relatively easy but still introduces edge cases, like apostrophes. \n",
            "Morphology and MWEs make tokenization more difficult because they require semantic knowledge beyond surface rules. \n",
            "Overall, tokenization is a balance between linguistic accuracy and tool consistency.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3 Question\n",
        "from collections import Counter\n",
        "\n",
        "def add_eow(word):\n",
        "    return \" \".join(list(word)) + \" _\"\n",
        "\n",
        "def get_stats(tokens):\n",
        "    pairs = Counter()\n",
        "    for word in tokens:\n",
        "        for i in range(len(word)-1):\n",
        "            pairs[(word[i], word[i+1])] += 1\n",
        "    return pairs\n",
        "\n",
        "def merge_pair(tokens, pair):\n",
        "    new_tokens = []\n",
        "    bigram = \" \".join(pair)\n",
        "    replacement = \"\".join(pair)\n",
        "    for word in tokens:\n",
        "        joined = \" \".join(word)\n",
        "        merged = joined.replace(bigram, replacement)\n",
        "        new_tokens.append(merged.split())\n",
        "    return new_tokens\n",
        "\n",
        "def segment(word, merges):\n",
        "    chars = list(word) + [\"_\"]\n",
        "    for merge in merges:\n",
        "        i = 0\n",
        "        while i < len(chars)-1:\n",
        "            if (chars[i], chars[i+1]) == merge:\n",
        "                chars[i:i+2] = [\"\".join(merge)]\n",
        "            else:\n",
        "                i += 1\n",
        "    return chars\n",
        "\n",
        "# ============================================================\n",
        "# 3.1 Manual BPE on Toy Corpus\n",
        "# ============================================================\n",
        "print(\"\\n=== 3.1 Manual BPE (Toy Corpus) ===\")\n",
        "toy_corpus = [\"low\",\"low\",\"low\",\"low\",\"low\",\"lowest\",\"lowest\",\n",
        "              \"newer\",\"newer\",\"newer\",\"newer\",\"newer\",\"newer\",\n",
        "              \"wider\",\"wider\",\"wider\",\"new\",\"new\"]\n",
        "\n",
        "corpus_tokens = [add_eow(w).split() for w in toy_corpus]\n",
        "\n",
        "# Step 1: compute stats\n",
        "pairs = get_stats(corpus_tokens)\n",
        "print(\"Bigram counts (top 5):\", pairs.most_common(5))\n",
        "\n",
        "# First 3 merges manually\n",
        "merges_manual = []\n",
        "for step in range(3):\n",
        "    pairs = get_stats(corpus_tokens)\n",
        "    best = pairs.most_common(1)[0][0]\n",
        "    merges_manual.append(best)\n",
        "    corpus_tokens = merge_pair(corpus_tokens, best)\n",
        "    print(f\"Step {step+1}: merge {best}, new vocab token: {''.join(best)}\")\n",
        "\n",
        "print(\"First 3 merges:\", merges_manual)\n",
        "\n",
        "# ============================================================\n",
        "# 3.2 Mini BPE Learner (Toy Corpus again)\n",
        "# ============================================================\n",
        "print(\"\\n=== 3.2 Mini BPE Learner (Toy Corpus) ===\")\n",
        "corpus_tokens = [add_eow(w).split() for w in toy_corpus]\n",
        "merges = []\n",
        "for i in range(10):\n",
        "    pairs = get_stats(corpus_tokens)\n",
        "    if not pairs: break\n",
        "    best = pairs.most_common(1)[0][0]\n",
        "    merges.append(best)\n",
        "    corpus_tokens = merge_pair(corpus_tokens, best)\n",
        "    print(f\"Step {i+1}: merge {best}, vocab size: {len(set(tok for word in corpus_tokens for tok in word))}\")\n",
        "\n",
        "# Segment sample words\n",
        "test_words = [\"new\", \"newer\", \"lowest\", \"widest\", \"newestest\"]\n",
        "print(\"\\nSegmentation examples:\")\n",
        "for w in test_words:\n",
        "    print(w, \"->\", segment(w, merges))\n",
        "\n",
        "# ============================================================\n",
        "# 3.3 Training BPE on Paragraph\n",
        "# ============================================================\n",
        "print(\"\\n=== 3.3 BPE on Paragraph ===\")\n",
        "paragraph = \"\"\"Artificial intelligence is transforming industries.\n",
        "Machine learning models are trained on vast amounts of data.\n",
        "They can recognize images, translate text, and even generate new content.\n",
        "However, ethical concerns remain. Fairness and bias are critical issues.\n",
        "Researchers continue to explore solutions.\"\"\"\n",
        "\n",
        "# Prepare corpus\n",
        "words = paragraph.lower().replace(\".\", \"\").split()\n",
        "corpus_para = [add_eow(w).split() for w in words]\n",
        "\n",
        "# Train 30 merges\n",
        "tokens = corpus_para\n",
        "merges_para = []\n",
        "for i in range(30):\n",
        "    pairs = get_stats(tokens)\n",
        "    if not pairs: break\n",
        "    best = pairs.most_common(1)[0][0]\n",
        "    merges_para.append(best)\n",
        "    tokens = merge_pair(tokens, best)\n",
        "\n",
        "print(\"Top 5 merges:\", merges_para[:5])\n",
        "\n",
        "# 5 longest tokens\n",
        "vocab = set(tok for word in tokens for tok in word)\n",
        "longest = sorted(vocab, key=len, reverse=True)[:5]\n",
        "print(\"Longest 5 tokens:\", longest)\n",
        "\n",
        "# Segment 5 words\n",
        "test_words_para = [\"artificial\", \"researchers\", \"bias\", \"transforming\", \"solutions\"]\n",
        "print(\"\\nSegmentation (Paragraph words):\")\n",
        "for w in test_words_para:\n",
        "    print(w, \"->\", segment(w, merges_para))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHoMUN0oigws",
        "outputId": "dc3a0037-3276-47a3-8748-17c9e822dbb9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== 3.1 Manual BPE (Toy Corpus) ===\n",
            "Bigram counts (top 5): [(('e', 'r'), 9), (('r', '_'), 9), (('w', 'e'), 8), (('n', 'e'), 8), (('e', 'w'), 8)]\n",
            "Step 1: merge ('e', 'r'), new vocab token: er\n",
            "Step 2: merge ('er', '_'), new vocab token: er_\n",
            "Step 3: merge ('n', 'e'), new vocab token: ne\n",
            "First 3 merges: [('e', 'r'), ('er', '_'), ('n', 'e')]\n",
            "\n",
            "=== 3.2 Mini BPE Learner (Toy Corpus) ===\n",
            "Step 1: merge ('e', 'r'), vocab size: 11\n",
            "Step 2: merge ('er', '_'), vocab size: 11\n",
            "Step 3: merge ('n', 'e'), vocab size: 11\n",
            "Step 4: merge ('ne', 'w'), vocab size: 11\n",
            "Step 5: merge ('l', 'o'), vocab size: 10\n",
            "Step 6: merge ('lo', 'w'), vocab size: 10\n",
            "Step 7: merge ('new', 'er_'), vocab size: 11\n",
            "Step 8: merge ('low', '_'), vocab size: 12\n",
            "Step 9: merge ('w', 'i'), vocab size: 11\n",
            "Step 10: merge ('wi', 'd'), vocab size: 10\n",
            "\n",
            "Segmentation examples:\n",
            "new -> ['new', '_']\n",
            "newer -> ['newer_']\n",
            "lowest -> ['low', 'e', 's', 't', '_']\n",
            "widest -> ['wid', 'e', 's', 't', '_']\n",
            "newestest -> ['new', 'e', 's', 't', 'e', 's', 't', '_']\n",
            "\n",
            "=== 3.3 BPE on Paragraph ===\n",
            "Top 5 merges: [('s', '_'), ('e', '_'), ('i', 'n'), ('a', 'r'), ('a', 'n')]\n",
            "Longest 5 tokens: ['trans', 'are_', 'ing_', 'ate_', 'es_']\n",
            "\n",
            "Segmentation (Paragraph words):\n",
            "artificial -> ['ar', 'ti', 'f', 'ic', 'i', 'al_']\n",
            "researchers -> ['re', 's', 'ear', 'ch', 'er', 's_']\n",
            "bias -> ['b', 'i', 'a', 's_']\n",
            "transforming -> ['trans', 'f', 'o', 'r', 'm', 'ing_']\n",
            "solutions -> ['s', 'o', 'l', 'u', 'ti', 'on', 's_']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4 Question Word Pair:\n",
        "Sunday → Saturday\n",
        "Tasks:\n",
        "\n",
        "#1. Find the minimum edit distance between Sunday and Saturday under both models:\n",
        "\n",
        "Model A (Sub = 1, Ins = 1, Del = 1)\n",
        "\n",
        "Model B (Sub = 2, Ins = 1, Del = 1)\n",
        "\n",
        "1) Minimum edit distance\n",
        "\n",
        "Model A (Sub=1, Ins=1, Del=1): Distance = 3\n",
        "\n",
        "Model B (Sub=2, Ins=1, Del=1): Distance = 4\n",
        "\n",
        "\n",
        "#2 . One valid edit sequence\n",
        "Model A (cost 3)\n",
        "\n",
        "Insert a after S → Saunday\n",
        "\n",
        "Insert t after Sa → Satunday\n",
        "\n",
        "Substitute n → r → Saturday\n",
        "\n",
        "Model B (cost 4)\n",
        "\n",
        "Insert a after S → Saunday\n",
        "\n",
        "Insert t after Sa → Satunday\n",
        "\n",
        "Delete n → Satuday\n",
        "\n",
        "Insert r before d → Saturday\n",
        "\n",
        "#3. Reflection (4–5 sentences)\n",
        "\n",
        "The two models gave different distances: Model A = 3, Model B = 4. In Model A, substitution was cheap and useful, so it was chosen for the mismatch.\n",
        "\n",
        "In Model B, substitution was expensive, so the algorithm preferred a delete + insert instead. The choice of model matters: for spell-checking, substitutions should be cheap since typos often involve wrong letters.\n",
        "\n",
        "For DNA alignment, substitutions (mutations) may be more costly or biologically significant, so making substitutions expensive produces more realistic alignments.\n",
        "     "
      ],
      "metadata": {
        "id": "nvIA0Ltlm52_"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}